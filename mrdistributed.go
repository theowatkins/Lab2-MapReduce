package main

import (
	"fmt"
	"strconv"
	"strings"
	"unicode"
)
import "./mr"
import "os"
import "log"
import "io/ioutil"
import "sort"

// for sorting by key.
type ByKey []mr.KeyValue

// for sorting by key.
func (a ByKey) Len() int           { return len(a) }
func (a ByKey) Swap(i, j int)      { a[i], a[j] = a[j], a[i] }
func (a ByKey) Less(i, j int) bool { return a[i].Key < a[j].Key }

func main() {
	if len(os.Args) < 2 {
		fmt.Fprintf(os.Stderr, "Usage: mrdistributed.go inputfiles...\n")
		os.Exit(1)
	}

	//
	// read each input file,
	// consolidate content of input files
	//
	intermediate := []mr.KeyValue{}
	var allContent string
	for _, filename := range os.Args[1:] {
		file, err := os.Open(filename)
		if err != nil {
			log.Fatalf("cannot open %v", filename)
		}
		content, err := ioutil.ReadAll(file)
		if err != nil {
			log.Fatalf("cannot read %v", filename)
		}
		file.Close()
		allContent = allContent + string(content)
	}

	// function to detect word separators.
	ff := func(r rune) bool { return !unicode.IsLetter(r) }

	// split content into an array of words.
	allWords := strings.FieldsFunc(allContent, ff)

	// split all content into shards
	const numShards = 100
	shardSize := len(allWords)/numShards
	leftova := len(allWords) - numShards * shardSize

	id := 0
    for s := 0; s < len(allWords); s += shardSize {
		shard := allWords[s:s+shardSize]
		go Map(shard, id)
		id++
	}
	if leftova > 0 {
		go Map(allWords[(numShards * shardSize):], id)
	}

	//
	// a big difference from real MapReduce is that all the
	// intermediate data is in one place, intermediate[],
	// rather than being partitioned into NxM buckets.
	//

	sort.Sort(ByKey(intermediate))

	oname := "mr-out-0"
	ofile, _ := os.Create(oname)

	//
	// call Reduce on each distinct key in intermediate[],
	// and print the result to mr-out-0.
	//
	i := 0
	for i < len(intermediate) {
		j := i + 1
		for j < len(intermediate) && intermediate[j].Key == intermediate[i].Key {
			j++
		}
		values := []string{}
		for k := i; k < j; k++ {
			values = append(values, intermediate[k].Value)
		}
		output := reducef(intermediate[i].Key, values)

		// this is the correct format for each line of Reduce output.
		fmt.Fprintf(ofile, "%v %v\n", intermediate[i].Key, output)

		i = j
	}

	ofile.Close()
}

func Shards(s string, shardSize int) []string {
    var shards []string
    shard := make([]rune, shardSize)
    len := 0
    for _, r := range s {
        shard[len] = r
        len++
        if len == shardSize {
            shards = append(shards, string(shard))
            len = 0
        }
    }
    if len > 0 {
        shards = append(shards, string(chunk[:len]))
    }
    return chunks
}

/*
 * Functions are added here to avoid using plugins. Plugins
 * are not currently supported on windows.
 */

func Map(filename string, contents string) []mr.KeyValue {

	kva := []mr.KeyValue{}
	for _, w := range words {
		kv := mr.KeyValue{w, "1"}
		kva = append(kva, kv)
	}
	return kva
}

//
// The reduce function is called once for each key generated by the
// map tasks, with a list of all the values created for that key by
// any map task.
//
func Reduce(key string, values []string) string {
	// return the number of occurrences of this word.
	return strconv.Itoa(len(values))
}

